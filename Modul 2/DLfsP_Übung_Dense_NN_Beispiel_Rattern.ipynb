{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLfsP Dense Neural Network am Beispiel Rattern\n",
    "Kurs Deep Learning für sequentielle Prozessdaten  \n",
    "\n",
    "#### In diesem Notebook wird das TensorFlow mit Dense Layern anhand des Anwendungsbeispiels Rattern geübt.\n",
    "\n",
    "Bei der Fertigung von Bauteilen treten manchmal störende Schwingungen auf, sog. Rattern. Dieses schädigt die Werkzeuge und führt zu einer niedrigeren Bauteilqualität.\n",
    "\n",
    "Im Datensatz \"Rattern\" werden die Drehzahl der Spindel und die Tiefe des Schnitts einer CNC-Fräse gemessen. Es soll ein Entscheidungsbaum erstellt werden. Dises soll vorhersagen können, bei welcher Kombination aus Drehzahl und Schnitttiefe das Rattern auftritt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Daten erfassen -> Daten erkunden -> Daten vorbereiten -> Modelle bilden -> Modelle validieren -> Modell testen & anwenden](Prozess_Modellentwicklung_v2.png \"model development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Bibliotheken importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Import der wichtigsten Pakete. '''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "#Einstellungen für die Grafikausgabe\n",
    "style = 'seaborn-whitegrid'\n",
    "plt.style.use(style)\n",
    "plt.rcParams.update({'font.size': 14})  # Schriftgröße aller Textzeichen im Graphen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Überprüfe, ob TensorFlow installiert ist und die GPU oder die CPU nutzt. '''\n",
    "print('TF version: ' + tf.__version__)\n",
    "if tf.test.is_built_with_gpu_support():\n",
    "    if len(tf.config.list_physical_devices('GPU'))==0:\n",
    "         print('Achtung, TensorFlow nutzt gerade die CPU und nicht die GPU!')\n",
    "    else:\n",
    "        print('Default GPU Device: {}'.format(tf.config.list_physical_devices('GPU')))\n",
    "        # GPU-Memory-Management:\n",
    "        config = tf.compat.v1.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        session = tf.compat.v1.Session(config=config)\n",
    "else:\n",
    "    print('TensorFlow CPU version active')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* Wähle eine Zahl zwischen 1 und 100 für die Generierung deiner spezifischen Zufallszahlen my_seed=\n",
    "\n",
    "Damit Ergebnisse Reproduzierbar sind, müssen mehrere Seeds für Zufallszahlengeneratoren gesetzt werden. \n",
    "\n",
    "AUSGABE:\n",
    "* Gewählte Zufallszahl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Setzen von Seeds, um Reproduzierbarkeit zu ermöglichen. '''\n",
    "# Erstelle eigene Zufallszahlen\n",
    "my_seed = TODO\n",
    "\n",
    "# Ausgabe gewählte Zufallszahlen\n",
    "print(\"\\nGewählte Zahl für Zufallszahlen: \\t\" + str(my_seed))\n",
    "\n",
    "# Seeds für diverse Zufallszahlengeneratoren setzen \n",
    "os.environ['PYTHONHASHSEED']=str(my_seed)\n",
    "tf.random.set_seed(my_seed)\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Daten erfassen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Daten mit pandas einlesen. \"\"\"\n",
    "df_train = pd.read_csv(\"Trainingsdaten_Rattern.csv\")\n",
    "df_test = pd.read_csv(\"Testdaten_Rattern.csv\")\n",
    "print('Daten erfolgreich importiert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Daten erkunden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Erste Zeilen des Trainingsdatensatzes als Beispiel. \"\"\"\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Statistische Beschreibung des Trainingsdatensatzes. \"\"\"\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Ausgabe Klassenzugehörigkeit. \"\"\"\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "surf = plt.scatter(df_train[\"Drehzahl Spindel\"],\n",
    "                   df_train[\"Tiefe des Schnitts\"],\n",
    "                   c=df_train[\"Rattern\"],\n",
    "                   cmap=plt.cm.coolwarm,\n",
    "                   s=100,\n",
    "                   alpha=0.7,\n",
    "                   edgecolors=\"black\")\n",
    "\n",
    "plt.colorbar(surf)\n",
    "plt.xlim(7750, 16250)\n",
    "plt.ylim(0, 0.023)\n",
    "plt.xlabel(\"Drehzahl Spindel\")\n",
    "plt.ylabel(\"Tiefe des Schnitts\")\n",
    "plt.title(\"Effekt des Ratterns über Schnitttiefe und Spindeldrehzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Daten vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Aufteilung in X und y. \"\"\"\n",
    "X_train = df_train.drop(columns=['Rattern']).to_numpy()\n",
    "y_train = df_train['Rattern'].to_numpy(dtype='int32')\n",
    "\n",
    "X_test = df_test.drop(columns=['Rattern']).to_numpy()\n",
    "y_test = df_test['Rattern'].to_numpy(dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Aufteilen in Trainings- und Validierungsdaten. \"\"\"\n",
    "# Festlegen des Anteils an Validierungsdaten\n",
    "validation_split = 0.15\n",
    "\n",
    "# Aufteilung der Daten\n",
    "X_train, X_val, y_train ,y_val = train_test_split(X_train, y_train, test_size=validation_split, shuffle=True, random_state=my_seed)\n",
    "\n",
    "# Ausgabe, Anzahl der Samples je Datensatz\n",
    "nr_train = len(X_train)\n",
    "nr_val = len(X_val)\n",
    "nr_test = len(X_test)\n",
    "print(nr_train, \"Trainingssequenzen\")\n",
    "print(nr_val, \"Validierungssequenzen\")\n",
    "print(nr_test, \"Testsequenzen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Daten normalisieren. \"\"\"\n",
    "# Importieren und Anlernen des Scalers (d.h. Scaler lernt Mittelwerte und Standardabweichungen der Features)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Normierung der Daten mit den gelernten Paramatern und gleichzeitiges Formatieren\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "print('Datenaufbereitung fertig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Überprüfen der Form der Numpy Arrays. '''\n",
    "print('Shape X_train: ' + str(X_train.shape))\n",
    "print('Shape X_val: ' + str(X_val.shape))\n",
    "print('Shape X_test: ' + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelle bilden\n",
    "### <span style=\"color:#0000ff;\">1.) Modellaufbau</span>\n",
    "\n",
    "TODO:\n",
    "* Zunächst muss die Modellstruktur definiert werden. \n",
    "* Die Anzahl der Layer und die Anzahl der Knoten/Nodes pro Layer ist einem selbst überlassen. \n",
    "* Probiere also verschiedene Modellgrößen aus, um das beste Modell für die Aufgabe zu finden.\n",
    "\n",
    "Nützliche Links:\n",
    "* https://keras.io/guides/sequential_model/   \n",
    "* https://keras.io/api/layers/core_layers/dense/   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Modellaufbau '''\n",
    "# Import der notwendigen Klassen und Funktionen\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense \n",
    "\n",
    "# Bestimmung der Form eines einzelnen Trainingsbeispiels\n",
    "input_shape = (2,) # Der Input in das Modell, ist ein 1D Tensor der Länge 2 \n",
    "\n",
    "# Definition des Modells\n",
    "model = Sequential()\n",
    "\"\"\"\n",
    "*\n",
    "* TODO: \n",
    "*      Dense Layers hinzufügen\n",
    "*     \n",
    "\"\"\"\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0000ff;\">2.) Modellkompilierung</span>\n",
    "\n",
    "Der Optimierer, die Loss Funktion und die Metrik wurden bereits festgelegt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Modell kompilieren. '''\n",
    "# Optimizer festlegen\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Einstellung für das spätere Training zum Kompilieren festlegen\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0000ff;\">3.) Modelltraining</span>  \n",
    "\n",
    "TODO:\n",
    "* Wähle die Anzahl der Epochen aus, für die das Modell trainiert werden soll. \n",
    "* Danach müssen die Trainingsinputs, -outputs sowie die Validierungsdaten bestimmt werden.\n",
    "\n",
    "Ausgabe: \n",
    "* Zwischenergebnisse je Epoche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' Modell trainieren '''\n",
    "\"\"\"\n",
    "*\n",
    "* TODO:\n",
    "*       Füge die fehlenden Variablen ein!\n",
    "*       (TODO löschen)\n",
    "*\n",
    "\"\"\"\n",
    "# Festlegung der Batch_Size und Anzahl der Epoche\n",
    "batch_size = 32\n",
    "epochs = TODO\n",
    "# Durchführen des Trainings. Die Ergebnisse je Epoche werden in der History gespeichert. \n",
    "history = model.fit(TODO, TODO, batch_size=batch_size, epochs=epochs,\n",
    "                    validation_data=(TODO, TODO), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelle validieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kennzahlen fürs Training und Validierung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Kennzahlen (Loss und Accuracy Metrik) fürs Training:')\n",
    "print(model.evaluate(X_train, y_train))\n",
    "print('Kennzahlen (Loss und Accuracy Metrik) für Validierung:')\n",
    "print(model.evaluate(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung des Trainingsverlaufs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Trainingsverlauf plotten. \"\"\"\n",
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1,len(train_acc) +1)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(epochs, train_acc,'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc,'b', label='Validation accuracy')\n",
    "\n",
    "plt.title('Accuracy', fontsize=16)\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(epochs, train_loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "\n",
    "plt.title('Loss', fontsize = 16)\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modell testen & anwenden\n",
    "\n",
    "Führen Sie diesen Schritt erst aus, wenn Sie mit Ihrem Modell zufrieden sind.  \n",
    "\n",
    "Der Test eines Modells soll ermitteln, wie gut das Modell übertragbar und generalisierbar ist.  \n",
    "Wenn Sie alle Modelle direkt testen, wählen Sie nur das Modell aus, was am Besten zum Testdatensatz passt. Aber nicht welches das Problem grundsätzlich am Besten löst. Sie Overfitten somit den Testdatensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Modell anhand Testdaten bewerten\"\"\"\n",
    "y_test_pred = (model.predict(X_test)>0.5).astype(\"int32\")\n",
    "# Berechne Genauigkeit auf den Testdaten\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Berechne den F1-Score auf den Testdaten\n",
    "f1score_test = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Ausgabe der Modellgenauigkeit\n",
    "print('Ergebnis für den Test:')\n",
    "print('Accuracy: \\t' + str(round(accuracy_test, 4)))\n",
    "print('F1-Score: \\t' + str(round(f1score_test, 4)))\n",
    "\n",
    "# Visualisierung der Konfusionsmatrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred)\n",
    "plt.grid()\n",
    "plt.title('Konfusionsmatrix auf Testdaten')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print('Classification Report')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modellvorhersagen visualisieren\n",
    "\n",
    "Da das Modell nur 2 Input-Parameter benötigt können wir alle möglichen Modellvorhersagen grafisch visualisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Visualisierung der Modellvorhersagen und Decision Boundary.'''\n",
    "# Erstelle Grid für Modellausgabe\n",
    "x = np.linspace(\n",
    "    X_train[:, 0].min(), X_train[:, 0].max(), 200\n",
    ")\n",
    "y = np.linspace(\n",
    "    X_train[:, 1].min(), X_train[:,1].max(), 200\n",
    ")\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Berechne Modellvorhersage\n",
    "z = pd.DataFrame({\"Drehzahl Spindel\": X.ravel(), \"Tiefe des Schnitts\": Y.ravel()})\n",
    "z = model.predict(z)\n",
    "\n",
    "# Ausgabe Modellvorhersage\n",
    "\n",
    "Z = np.asarray(z).reshape(200, 200)\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "plt.title(\"Modellvorhersage und Datensätze (Train = Kreis, VAlidierung = Viereck, Test = Raute)\")\n",
    "plt.xlabel(\"Drehzahl Spindel\")\n",
    "plt.ylabel(\"Tiefe des Schnitts\")\n",
    "surf = plt.contourf(X, Y, Z, cmap=plt.cm.coolwarm)\n",
    "plt.colorbar(surf)\n",
    "plt.scatter(\n",
    "    X_train[:, 0],\n",
    "    X_train[:, 1],\n",
    "    c=y_train,\n",
    "    marker=\"o\",\n",
    "    alpha=0.3,\n",
    "    edgecolors=\"black\",\n",
    "    s=70)\n",
    "plt.scatter(\n",
    "    X_val[:, 0],\n",
    "    X_val[:, 1],\n",
    "    c=y_val,\n",
    "    marker=\"s\",\n",
    "    edgecolors=\"black\",\n",
    "    s=70)\n",
    "plt.scatter(\n",
    "    X_test[:, 0],\n",
    "    X_test[:, 1],\n",
    "    c=y_test,\n",
    "    marker=\"D\",\n",
    "    edgecolors=\"black\",\n",
    "    s=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bonus: Modell mit der Functional API definieren\n",
    "\n",
    "Die functional API ist praktisch für komplexere Modellarchitekturen.  \n",
    "\n",
    "https://keras.io/guides/functional_api/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Functional Model\"\"\"\n",
    "\n",
    "inputs = keras.Input(input_shape, dtype=\"float32\")\n",
    "\"\"\"\n",
    "*\n",
    "* TODO: \n",
    "*      Dense Layers hinzufügen\n",
    "*            \n",
    "*     \n",
    "*\n",
    "\"\"\"\n",
    "\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DLfsP)",
   "language": "python",
   "name": "python-dlfsp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
