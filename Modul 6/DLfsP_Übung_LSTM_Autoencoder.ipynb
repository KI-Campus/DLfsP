{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLfsP LSTM Autoencoder am Beispiel zyklischer Prozessdaten\n",
    "Kurs Deep Learning für sequentielle Prozessdaten\n",
    "\n",
    "#### In diesem Notebook wird ein LSTM Autoencoder auf Zeitreihendaten eines zyklischen Prozesses angewendet. \n",
    "\n",
    "Die Daten stammen von einer echten Produktionsanlage mussten jedoch für die Verwendung hier anonymisiert werden. Beim Prozess handelt es sich um einen zyklischen Prozess, wobei ein Zyklus, hier Takt genannt, etwa 2,7 Sekunden dauert. Der Prozesszyklus soll anhand des Motorstroms des Hauptantriebes überwacht werden. Dazu wurden einige anomale Zyklen bereitgestellt, diese dienen zur Entwicklung Anomaliedetektion.\n",
    "\n",
    "Ziel der Aufgabe ist somit, einen LSTM Autoencoder zu entwickeln, der den Prozessverlauf von guten Zyklen erlernt hat.  \n",
    "Darauf aufbauend soll eine Anomaliedetektion entworfen werden. \n",
    "\n",
    "__Hinweis:__ Auch wenn die Aufgabe LSTM Autoencoder heißt, dürfen auch andere rekurrente Schichten wie GRU verwendet werden. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-Mining-Prozess:\n",
    "![Daten erfassen -> Daten erkunden -> Daten vorbereiten -> Modelle bilden -> Modelle validieren -> Modell testen & anwenden](Prozess_Modellentwicklung_v2.png \"model development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Bibliotheken importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import libraries\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import randint\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, precision_recall_curve\n",
    "import seaborn as sns\n",
    "\n",
    "#Einstellungen für die Grafikausgabe\n",
    "style = 'seaborn-whitegrid'\n",
    "plt.style.use(style)\n",
    "plt.rcParams.update({'font.size': 14})  # Schriftgröße aller Textzeichen im Graphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use TF with GPU or CPU ?\n",
    "print('\\nTF version: ' + tf.__version__)\n",
    "if tf.test.is_built_with_gpu_support():\n",
    "    if len(tf.config.list_physical_devices('GPU'))==0:\n",
    "         print('Please install GPU version of TF\\n' +\n",
    "                  'TF is currently using the CPU')\n",
    "    else:\n",
    "        print('Default GPU Device: {}'.format(tf.config.list_physical_devices('GPU')))\n",
    "        # GPU-Memory-Management:\n",
    "        config = tf.compat.v1.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        session = tf.compat.v1.Session(config=config)\n",
    "else:\n",
    "    print('TF CPU version active')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* Wähle eine Zahl zwischen 1 und 100 für die Generierung deiner spezifischen Zufallszahlen my_seed=\n",
    "\n",
    "Damit Ergebnisse Reproduzierbar sind, müssen mehrere Seeds für Zufallszahlengeneratoren gesetzt werden. \n",
    "\n",
    "AUSGABE:\n",
    "* Gewählte Zufallszahl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Setzen von Seeds, um Reproduzierbarkeit zu ermöglichen. '''\n",
    "# Erstelle eigene Zufallszahlen\n",
    "my_seed = 2\n",
    "\n",
    "# Ausgabe gewählte Zufallszahlen\n",
    "print(\"\\nGewählte Zahl für Zufallszahlen: \\t\" + str(my_seed))\n",
    "\n",
    "# Seeds für diverse Zufallszahlengeneratoren setzen \n",
    "os.environ['PYTHONHASHSEED']=str(my_seed)\n",
    "tf.random.set_seed(my_seed)\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Daten erfassen\n",
    "Bei den Daten handelt es sich wieder um CSV Dateien. Diese wurden jedoch aus Effizienzgründen gezippt.   \n",
    "Jedoch ist Pandas in der Lage, direkt die Daten aus den gezippten Dateien zu importieren.   \n",
    "Dazu ist der Zusatzbefehl \"compression='zip'\" notwendig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Daten importieren'''\n",
    "df = pd.read_csv('zykl_Produktionsprozess_gut.zip', compression='zip') \n",
    "df_anomaly = pd.read_csv('zykl_Produktionsprozess_anomal.zip', compression='zip') \n",
    "print('Daten erfolgreich importiert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Daten erkunden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ersten Datensätze als Beispiel\"\"\"\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Statistische Beschreibung Datensatz\"\"\"\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Zyklen plotten. \"\"\"\n",
    "# Auswahl zufälliger Zyklen \n",
    "cycles = random.sample(df['Takt'].drop_duplicates().to_list(), 3)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title('Motorstrom')\n",
    "for c in cycles:\n",
    "    plt.plot(df[df['Takt']==c]['time_ms'],df[df['Takt']==c]['Motorstrom'],'o-',label='Takt Nr. '+str(int(c)))\n",
    "\n",
    "plt.ylabel('Motorstrom')\n",
    "plt.xlabel('Zykluszeit in ms')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Daten vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Aufteilung in X und y\"\"\"\n",
    "# Maximale Zykluslänge\n",
    "maxlen = df['Takt'].value_counts().max()    \n",
    "print(maxlen)\n",
    "\n",
    "# Aufteilung X und y Daten als DataFrames\n",
    "# Daten ohne Anomalien\n",
    "y_df = df[['Status','Takt']]\n",
    "X_df = df.reset_index(drop=True)\n",
    "# Anomalien\n",
    "y_df_anomaly = df_anomaly[['Status','Takt']]\n",
    "X_df_anomaly = df_anomaly.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Umwandeln der DataFrames in numpy arrays und group by run_ID\"\"\"\n",
    "# Daten ohne Anomalien\n",
    "g = X_df.groupby('Takt').cumcount()\n",
    "X = (X_df.set_index(['Takt',g])\n",
    "     .unstack()\n",
    "     .stack().groupby(level=0)\n",
    "     .apply(lambda x: x['Motorstrom'].values)\n",
    "     .to_numpy())\n",
    "g = y_df.groupby('Takt').cumcount()\n",
    "y = (y_df.set_index(['Takt',g])\n",
    "     .unstack()\n",
    "     .stack().groupby(level=0)\n",
    "     .apply(lambda x: np.rint(x.sum()/len(x)))\n",
    "     .to_numpy().astype(\"int32\"))\n",
    "\n",
    "print('Shapes der numpy Arrays X und y:')\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# Anomalien\n",
    "g = X_df_anomaly.groupby('Takt').cumcount()\n",
    "X_anomaly = (X_df_anomaly.set_index(['Takt',g])\n",
    "     .unstack()\n",
    "     .stack().groupby(level=0)\n",
    "     .apply(lambda x: x['Motorstrom'].values)\n",
    "     .to_numpy())\n",
    "g = y_df_anomaly.groupby('Takt').cumcount()\n",
    "y_anomaly = (y_df_anomaly.set_index(['Takt',g])\n",
    "     .unstack()\n",
    "     .stack().groupby(level=0)\n",
    "     .apply(lambda x: np.rint(x.sum()/len(x)))\n",
    "     .to_numpy().astype(\"int32\"))\n",
    "\n",
    "print('Shapes der numpy Arrays X_anomaly und y_anomaly:')\n",
    "print(X_anomaly.shape, y_anomaly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Auffüllen der ungleichen Messreihen'''\n",
    "# Auffüllen mit führenden Nullen, damit die Zeitreihen die gleiche Länge haben\n",
    "X_pre = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=maxlen, padding='pre', dtype=\"float32\")\n",
    "X_anomaly_pre = tf.keras.preprocessing.sequence.pad_sequences(X_anomaly, maxlen=maxlen, padding='pre', dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Aufteilen in Trainings-und Testdaten'''\n",
    "# Hinweis: Für die Validierung wird dieses Mal eine direkte Variante von Tensorflow verwendet\n",
    "# Festlegen des Anteils an Testdaten \n",
    "test_split = 0.2\n",
    "\n",
    "# Aufteilung der Daten\n",
    "# Testdaten abspalten# Aufteilung der Daten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pre, y, \n",
    "                                                    test_size=test_split, \n",
    "                                                    shuffle=True, \n",
    "                                                    random_state=my_seed)\n",
    "# Kombinieren der Testdaten und Anomaliedaten zu einem Datensatz für die Anomaliedetektion (AD)\n",
    "X_AD = np.concatenate((X_test, X_anomaly_pre), axis=0)\n",
    "y_AD = np.concatenate((y_test, y_anomaly))\n",
    "\n",
    "nr_train=len(X_train)\n",
    "nr_AD=len(X_AD)\n",
    "print(nr_train, \"Training sequences\")\n",
    "print(nr_AD, \"Anomaliedetektion sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Formatierung der Daten'''\n",
    "X_train = X_train.reshape(nr_train, maxlen, 1)\n",
    "X_AD = X_AD.reshape(nr_AD, maxlen, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Überprüfen der Form der Numpy Arrays. '''\n",
    "print('Shape X_train:\\t' + str(X_train.shape))\n",
    "print('Shape X_AD:\\t' + str(X_AD.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelle bilden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.) Modellaufbau des LSTM Autoencoders\n",
    "\n",
    "Erstelle ein Decoder-Encoder Modell das als Autoencoder verwendet werden soll. \n",
    "\n",
    "TODO:\n",
    "* Importiere den notwendigen Layer\n",
    "* Definiere eine Modellstruktur \n",
    "* Die Anzahl der Layer und die Anzahl der Knoten/Nodes pro Layer ist einem selbst überlassen \n",
    "* Probiere also verschiedene Modellgrößen aus, um das beste Modell für die Aufgabe zu finde\n",
    "\n",
    "__Hinweis:__  \n",
    "* Anstatt LSTM Schichten kann auch gerne eine GRU Schicht verwendet werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Modellaufbau'''\n",
    "# Import der notwendigen Schichten\n",
    "from tensorflow.keras.layers import LSTM, RepeatVector, TimeDistributed, Dense\n",
    "\n",
    "# Definition des Input_Shapes\n",
    "input_shape = (276, 1)\n",
    "\n",
    "# Definition des Modells\n",
    "model=Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, input_shape=input_shape))\n",
    "model.add(LSTM(32))\n",
    "model.add(RepeatVector(n=276))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) Modellkompilierung des LSTM Autoencoder Modells\n",
    "TODO: \n",
    "* Wähle einen Optimierer aus (Adam, Nadam, RMSprop oder SGD) \n",
    "* Lege - falls notwendig - die Parameter des Optimiers fest, Lernrate, GradientClipping o.ä.\n",
    "* Bestimme, welche Loss Funktion verwendet werden soll \n",
    "\n",
    "Hinweise:  \n",
    "Die Lernrate ist ein sehr wichtiger Hyperparameter, der viel Einfluss auf den Verlauf und Schnelligkeit des Trainings hat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Modell kompilieren. '''\n",
    "# Optimizer festlegen\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Einstellung für das spätere Training zum Kompilieren festlegen\n",
    "model.compile(optimizer=optimizer, loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.) Trainieren des LSTM Autoencoder Modells\n",
    "\n",
    "TODO:\n",
    "* Wähle die Anzahl der Epochen aus, für die das Modell trainiert werden soll  \n",
    "* Danach müssen die Trainingsinputs, -outputs sowie die Validierungsdaten bestimmt werden  \n",
    "* Anstatt Tensorflow die Validierungsdaten zu geben, soll dies Tensorflow selbstständig machen. Stelle dazu den validation_split auf einen geeigneten Wert (z.B.  0.1)\n",
    "\n",
    "Hinweis:  \n",
    "* Nicht vergessen, die Zielgröße beim Autoencoder ist nicht der Status, der in \"y_train\" abgespeichert worden ist, sondern die Rekonstruktion von X_train. \n",
    "\n",
    "Ausgabe: \n",
    "* Zwischenergebnisse je Epoche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' Modell trainieren '''\n",
    "# Festlegung der Batch_Size und Anzahl der Epoche\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "\n",
    "history = model.fit(X_train, X_train, validation_split=0.1, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Modelle validieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Trainingsverlauf plotten. \"\"\"\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "epochs=range(1,len(loss) +1)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(epochs, loss, 'b-x', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.xlim((0, max(epochs)))\n",
    "plt.ylim(bottom=0.0)  \n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Training loss Autoencoder Model')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vergleich Vorhersage vs. echter Verlauf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Plotten echter Zyklus und vorhergesaten Zyklus'''\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(X_train[1,:,:], label='Original Zyklus')\n",
    "plt.plot(model.predict(X_train)[1], label='Vorhersage')\n",
    "plt.xlabel('Zeit in ms')\n",
    "plt.ylabel('Motorstrom')\n",
    "plt.title('Vorhersage vs. echter Verlauf')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain Autoencoder\n",
    "\n",
    "Nach dem finden der besten Hyperparameter für das Autoencoder Modell ist es sinnvoll, diesen nochmal mit allen Daten (Training und Validierung) zu trainieren. Dies sollte die folgenden Ergebnisse nochmal verbessern. \n",
    "Hiwer wird aber aus Zeitgründen darauf verzichtet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaliedetektion\n",
    "\n",
    "Führe diesen Schritt erst aus, wenn das erstellt Modell gut genug performt.\n",
    "\n",
    "Bei der Anomaliedetektion soll der LSTM Autoencoder verwendet werden, um Anomalien vorhzusagen. Dafür muss ein Rekonstruktionsfehler ausgewählt werden und ein Threshold für den Rekonstruktionsfehler bestimmt werden.   \n",
    "Zur Hilfe wird hier die Precision Recall Kurve für verschiedene Thresholds geplottet und die Verteilung der Fehler geplottet. \n",
    "\n",
    "TODO: \n",
    "* Wähle einen Rekonstruktionsfehler aus den zwei vorgegebenen, die mittlere absolute Abweichung oder die maximale absolute Abweichung\n",
    "* Bestimme einen Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Histogramm der Fehler für Trainingsdaten'''\n",
    "# Berechnung der Vorhersagen für die Trainingsdaten\n",
    "X_train_pred = model.predict(X_train)\n",
    "\n",
    "# Berechnung der Rekonstruktionsfehler (zwei Varianten)\n",
    "train_max_abs_error = np.max(np.abs(X_train_pred - X_train),axis=1).flatten()\n",
    "train_mean_abs_error = np.mean(np.abs(X_train_pred - X_train),axis=1).flatten()\n",
    "\n",
    "# Auswahl bzw. Übergabe eines Rekonstruktionsfehlers\n",
    "train_error = TODO\n",
    "\n",
    "# Plot Verteilung der Rekonstruktionsfehler\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.hist(train_error, bins=50)\n",
    "plt.xlabel(\"Train Rekonstruktionsfehler\")\n",
    "plt.ylabel(\"Häufigkeit\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Erstellung einer Precision Recall Kurve '''\n",
    "# Vorhersage der Verläufe für den X_AD Datensatz\n",
    "X_AD_pred = model.predict(X_AD)\n",
    "\n",
    "max_abs_error = np.max(np.abs(X_AD_pred - X_AD),axis=1).flatten()\n",
    "mean_abs_error = np.mean(np.abs(X_AD_pred - X_AD),axis=1).flatten()\n",
    "\n",
    "AD_error = TODO\n",
    "error_df = pd.DataFrame({'reconstruction_error':AD_error,\n",
    "                        'True_class':y_AD.flatten().tolist()})\n",
    "\n",
    "print(error_df.head(5))\n",
    "\n",
    "precision, recall, threshold = precision_recall_curve(error_df.True_class, error_df.reconstruction_error)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title('Precision and Recall for different threshold values')\n",
    "plt.plot(threshold, precision[1:],label=\"Precision\")\n",
    "plt.plot(threshold, recall[1:],label=\"Recall\")\n",
    "plt.xlim(left=0.0) \n",
    "plt.ylim(0.0,1.01)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Precision/Recall')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Auswahl eines Thresholds '''\n",
    "# Der threshold (dt. Grenzwert) bestimmt, ab welchen Fehlergröße ein Verlauf als Anomalie eingestuft wird\n",
    "threshold = TODO\n",
    "print(\"Reconstruction error threshold:\", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Anomaliedetektion bewerten \"\"\"\n",
    "# Plot Verteilung aller Rekonstruktionsfehlerwerte\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.hist(AD_error, bins=50)\n",
    "plt.xlabel(\"AD Rekonstruktionsfehler\")\n",
    "plt.ylabel(\"Häufigkeit\")\n",
    "plt.show()\n",
    "\n",
    "# Bestimmung der vorhergesagten Anomalien\n",
    "y_AD_pred = (AD_error > threshold).astype(\"int32\")\n",
    "\n",
    "# Berechne Genauigkeit auf den Anomaliedetektionsdaten\n",
    "accuracy_test = accuracy_score(y_AD, y_AD_pred)\n",
    "\n",
    "# Berechne den F1-Score auf den Anomaliedetektionsdaten\n",
    "f1score_test = f1_score(y_AD, y_AD_pred)\n",
    "\n",
    "# Ausgabe der Modellgenauigkeit\n",
    "print('Ergebnis für den Test:')\n",
    "print('Accuracy: \\t' + str(round(accuracy_test, 4)))\n",
    "print('F1-Score: \\t' + str(round(f1score_test, 4)))\n",
    "\n",
    "# Visualisierung der Konfusionsmatrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_AD, y_AD_pred)\n",
    "plt.grid()\n",
    "plt.title('Konfusionsmatrix auf Testdaten')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print('Classification Report')\n",
    "print(classification_report(y_AD, y_AD_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (DLfsP)",
   "language": "python",
   "name": "python-dlfsp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
